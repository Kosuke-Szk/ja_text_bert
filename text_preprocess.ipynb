{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語wikiからコーパスを作成するスクリプトです.<br>\n",
    "https://dumps.wikimedia.org/jawiki/latest/ <br>\n",
    "こちらのサイトから最新版の\"pages-articles\"のアドレスを手に入れてください. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダンプデータには不要なマークアップなどが含まれているので、取り除くためのテキストクリーニング用のスクリプトをgitから持ってきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/attardi/wikiextractor.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語wikiに対してテキストクリーニングを実行します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python wikiextractor/WikiExtractor.py -o extracted jawiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストに前処理を加えた上で,複数のtxtファイルをひとつに結合します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('./tmp.txt','w') as f:\n",
    "    for directory in glob.glob('./extracted/*'):\n",
    "        for name in glob.glob(directory+'/*'):\n",
    "            with open(name, 'r') as r:\n",
    "                for line in r:\n",
    "                    # titleを削除する\n",
    "                    if '<doc ' in line:\n",
    "                        next(r)\n",
    "                        next(r)\n",
    "                    elif '</doc>' in line:\n",
    "                        f.write('\\n')\n",
    "                        continue\n",
    "                    else:\n",
    "                        # 空白・改行削除、大文字を小文字に変換\n",
    "                        text = BeautifulSoup(line.strip()).text.lower()\n",
    "                        f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからはBERTのトレーニング用にテキストファイルを整形していきます.<br>\n",
    "文章を単語ごとに分割し, ひとつの単元の中に偶数個の文章が含まれるように調整します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import random\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "filename = 'tmp.txt'\n",
    "save_file = 'even_rows100M.txt'\n",
    "LIMIT_BYTE = 100000000 # 100Mbyte\n",
    "# t = MeCab.Tagger('-Owakati') # Neologdを辞書に使っている人場合はそちらを使用するのがベターです\n",
    "t = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/ -Owakati')\n",
    "\n",
    "def get_byte_num(s):\n",
    "    return len(s.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_file, 'w') as f:\n",
    "    count_byte = 0\n",
    "    with open(filename) as r:\n",
    "        for text in r:\n",
    "            print('{} bytes'.format(count_byte))\n",
    "            text = t.parse(text).strip()\n",
    "            # 一文ごとに分割する\n",
    "            text = text.split('。')\n",
    "            # 空白要素は捨てる\n",
    "            text = [t.strip() for t in text if t]\n",
    "            # 一単元の文書が偶数個の文章から成るようにする(BERTのデータセットの都合上)\n",
    "            max_text_len = len(text) // 2\n",
    "            text = text[:max_text_len * 2]\n",
    "            text = '\\n'.join(text)\n",
    "            f.write(text)\n",
    "            count_byte += get_byte_num(text)\n",
    "            if count_byte >= LIMIT_BYTE:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでBERTの学習に使うデータセットができました.<br>\n",
    "今度はTraining用とValidation用のデータに分割します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = sum(1 for line in open(save_file))\n",
    "print('Base file lines : ', num_lines)\n",
    "# 全体の80%をTraining dataに当てます\n",
    "train_lines = int(num_lines * 0.8)\n",
    "print('Train file lines : ', train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_name_temp = './data/splitted_%d.txt'\n",
    "\n",
    "split_index = 1\n",
    "line_index = 1\n",
    "out_file = open(out_file_name_temp % (split_index,), 'w')\n",
    "in_file = open(save_file)\n",
    "line = in_file.readline()\n",
    "while line:\n",
    "    if line_index > train_lines:\n",
    "        print('Starting file: %d' % split_index)\n",
    "        out_file.close()\n",
    "        split_index = split_index + 1\n",
    "        line_index = 1\n",
    "        out_file = open(out_file_name_temp % (split_index,), 'w')\n",
    "    out_file.write(line)\n",
    "    line_index = line_index + 1\n",
    "    line = in_file.readline()\n",
    "    \n",
    "out_file.close()\n",
    "in_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train file lines : ', sum(1 for line in open('./data/splitted_1.txt')))\n",
    "print('Valid file lines : ', sum(1 for line in open('./data/splitted_2.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これにてテキストの前処理は完了です！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
